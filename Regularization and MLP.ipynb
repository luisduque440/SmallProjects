{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of ten features and one target variable.  More information is found at http://scikit-learn.org/stable/datasets/index.html#diabetes-dataset\n",
    "\n",
    "After importing the datasets, add additional interaction variables to the features matrix.  You should have 55 variables (including 45 interaction variables) and one target variable. Add one term for every pair of variables; in practice one may introduce only a few select interaction terms based on domain knowledge or experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The following code will split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 20** and **test_size = 0.33**.  Note: lambda is called alpha in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric. (5pts)\n",
    "\n",
    "2) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? (4pts)\n",
    "\n",
    "3) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MSE) on the test data for each. (3pts)\n",
    "\n",
    "4) Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for 55 variables. What do you observe from these coefficients? (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, interaction_only=True, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyungwoo/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lasso alpha: 0.00660970574233\n",
      "Best ridge alpha: 0.005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_folds = 5\n",
    "k_fold = cross_validation.KFold(len(X_train), n_folds)\n",
    "lasso_alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "lasso_avg_rmse = {}\n",
    "\n",
    "for alpha in lasso_alphas:\n",
    "    lasso = linear_model.Lasso(alpha=alpha)\n",
    "    avg_mse = 0\n",
    "    for k, (train, test) in enumerate(k_fold):\n",
    "        lasso.fit(X_train[train], y_train[train])\n",
    "        avg_mse = avg_mse + mean_squared_error(y_train[test], lasso.predict(X_train[test])) \n",
    "    lasso_avg_rmse[alpha] = avg_mse / n_folds\n",
    "best_alpha_lasso = min(lasso_avg_rmse, key=lasso_avg_rmse.get)\n",
    "print(\"Best lasso alpha: {}\".format(best_alpha_lasso))\n",
    "\n",
    "\n",
    "ridge_alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "ridge_avg_rmse = {}\n",
    "\n",
    "for alpha in ridge_alphas:\n",
    "    ridge = linear_model.Ridge(alpha=alpha)\n",
    "    avg_mse = 0\n",
    "    for k, (train, test) in enumerate(k_fold):\n",
    "        ridge.fit(X_train[train], y_train[train])\n",
    "        avg_mse = avg_mse + mean_squared_error(y_train[test], ridge.predict(X_train[test])) \n",
    "    ridge_avg_rmse[alpha] = avg_mse / n_folds\n",
    "best_alpha_ridge = min(ridge_avg_rmse, key=ridge_avg_rmse.get)\n",
    "print(\"Best ridge alpha: {}\".format(best_alpha_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "ridge = linear_model.Ridge(alpha=alpha)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "lasso = linear_model.Lasso(alpha=alpha)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso tends to zero out the feature coefficients while Ridge decreases the magnitude of the feature coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for linear: 3377.94455926\n",
      "MSE for ridge: 2959.81880664\n",
      "MSE for lasso: 2972.25999624\n"
     ]
    }
   ],
   "source": [
    "#%% (3) - train and test MSE\n",
    "linear_clf = linear_model.LinearRegression()\n",
    "ridge_clf = linear_model.Ridge(alpha=best_alpha_ridge)\n",
    "lasso_clf = linear_model.Lasso(alpha=best_alpha_lasso)\n",
    "\n",
    "def fit_reg(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(pred, y_test)\n",
    "    print(\"MSE for {0}: {1}\".format(name, mse))\n",
    "\n",
    "fit_reg(linear_clf, \"linear\")\n",
    "fit_reg(ridge_clf, \"ridge\")\n",
    "fit_reg(lasso_clf, \"lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00598825050149\n"
     ]
    }
   ],
   "source": [
    "lassocv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lassocv.fit(X_train, y_train)\n",
    "print lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2977.9684786\n"
     ]
    }
   ],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "print mean_squared_error(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       -8.945181\n",
      "1     -197.429177\n",
      "2      565.596546\n",
      "3      346.757826\n",
      "4     -946.194163\n",
      "5      732.361940\n",
      "6       66.314443\n",
      "7        0.000000\n",
      "8      826.206133\n",
      "9        6.442448\n",
      "10    2520.577652\n",
      "11       0.000000\n",
      "12       0.000000\n",
      "13       0.000000\n",
      "14      -0.000000\n",
      "15       0.000000\n",
      "16       0.000000\n",
      "17    1532.331575\n",
      "18      -0.000000\n",
      "19       0.000000\n",
      "20     682.004315\n",
      "21       0.000000\n",
      "22       0.000000\n",
      "23       0.000000\n",
      "24   -1215.558781\n",
      "25       0.000000\n",
      "26       0.000000\n",
      "27    1076.317504\n",
      "28       0.000000\n",
      "29       0.000000\n",
      "30      -0.000000\n",
      "31       0.000000\n",
      "32       0.000000\n",
      "33    1151.350615\n",
      "34      80.803344\n",
      "35       0.000000\n",
      "36     682.635731\n",
      "37      -0.000000\n",
      "38       0.000000\n",
      "39      -0.000000\n",
      "40       0.000000\n",
      "41       0.000000\n",
      "42      -0.000000\n",
      "43       0.000000\n",
      "44       0.000000\n",
      "45      -0.000000\n",
      "46      -0.000000\n",
      "47     585.751561\n",
      "48       0.000000\n",
      "49      -0.000000\n",
      "50       0.000000\n",
      "51       0.000000\n",
      "52       0.000000\n",
      "53     966.133960\n",
      "54       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print pd.Series(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the coefficients are now reduced to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron Regressor\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the Boston house-prices dataset for this problem http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y.\n",
    "\n",
    "Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=40, learning_rate_init = 0.005\n",
    "    \n",
    "Now, consider three different settings for the number of hidden units:\n",
    "    \n",
    "   (a) *hidden_layer_sizes = (2,)* (b) *hidden_layer_sizes = (8,)* (c) *hidden_layer_sizes = (15,)*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (a), (b), and (c) (5pts)\n",
    "      \n",
    "2) Now, using the same number of hidden units used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works the best, (a), (b), or (c)? Briefly analyze the result in terms of the number of hidden units. (3pts)\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves for model (a), (b), and (c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_* value. (2pts)\n",
    "\n",
    "4) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=40, hidden_layer_sizes = (15,)\n",
    "    \n",
    "Now, consider three different settings for the learning rates:\n",
    "    \n",
    "   (i) *learning_rate_init = 0.005* (ii) *learning_rate_init = 0.01* (iii) *learning_rate_init = 1*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (i), (ii), and (iii) (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "data = datasets.load_boston()\n",
    "feature_cols = data.feature_names\n",
    "X = pd.DataFrame(data.data, columns = feature_cols)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hidden layer: (2,)>\n",
      " Average RMSE (CV): 5.65299177605\n",
      "\n",
      "<hidden layer: (8,)>\n",
      " Average RMSE (CV): 4.45561264758\n",
      "\n",
      "<hidden layer: (15,)>\n",
      " Average RMSE (CV): 3.82804387751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_hidden_layers = [(2,),(8,),(15,)]\n",
    "\n",
    "n_folds = 5\n",
    "k_folds = KFold(n_splits=n_folds)\n",
    "lrate = 0.005\n",
    "\n",
    "for param_hidden in param_hidden_layers:\n",
    "    aRMSE = 0\n",
    "    for train_idx, test_idx in k_folds.split(X_train):\n",
    "        regMLP = MLPRegressor(activation = 'tanh',\n",
    "                              solver = 'sgd',\n",
    "                              learning_rate='constant',\n",
    "                              random_state=42,\n",
    "                              batch_size=40,\n",
    "                              learning_rate_init = lrate,\n",
    "                              hidden_layer_sizes = param_hidden)\n",
    "        stdScaler = StandardScaler()\n",
    "        xTrain = stdScaler.fit_transform(X_train.iloc[train_idx,:])\n",
    "        yTrain = y_train.iloc[train_idx]\n",
    "            \n",
    "        regMLP.fit(xTrain,yTrain)\n",
    "        aRMSE += np.sqrt(mean_squared_error(y_train.iloc[test_idx],\n",
    "                                            regMLP.predict(stdScaler.transform(X_train.iloc[test_idx,:]))))/n_folds\n",
    "    \n",
    "    print '<hidden layer: {}>\\n Average RMSE (CV): {}\\n'.format(param_hidden,aRMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hidden layer: (2,)>\n",
      " RMSE (Train) = 5.02382771248, RMSE (Test) = 4.96115622735\n",
      "\n",
      "<hidden layer: (8,)>\n",
      " RMSE (Train) = 2.55815487937, RMSE (Test) = 4.27238345715\n",
      "\n",
      "<hidden layer: (15,)>\n",
      " RMSE (Train) = 1.96487751008, RMSE (Test) = 3.93351931643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrate = 0.005\n",
    "param_hidden_layers = [(2,),(8,),(15,)]\n",
    "\n",
    "RMSE_train_test = {}\n",
    "regMLPs = {}\n",
    "for param_hidden in param_hidden_layers:\n",
    "    regMLP = MLPRegressor(activation = 'tanh',\n",
    "                          solver = 'sgd',\n",
    "                          learning_rate='constant',\n",
    "                          random_state=42,\n",
    "                          batch_size=40,\n",
    "                          learning_rate_init = lrate,\n",
    "                          hidden_layer_sizes = param_hidden)\n",
    "    stdScaler = StandardScaler()\n",
    "    xTrain = stdScaler.fit_transform(X_train)\n",
    "    regMLP.fit(xTrain,y_train)\n",
    "    RMSE_train_test[param_hidden] = [np.sqrt(mean_squared_error(y_train,regMLP.predict(xTrain))),\n",
    "                                     np.sqrt(mean_squared_error(y_test,regMLP.predict(stdScaler.transform(X_test))))]\n",
    "    \n",
    "    regMLPs[param_hidden] = regMLP\n",
    "    \n",
    "    print '<hidden layer: {}>\\n RMSE (Train) = {}, RMSE (Test) = {}\\n'.format(param_hidden,\n",
    "                                                                              RMSE_train_test[param_hidden][0],\n",
    "                                                                              RMSE_train_test[param_hidden][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP regressor with 15 hidden units performed better than the other two models. Increasing number of hidden units is directly related to the complexity of model, and this regression problem was better fit by the more complex model.\n",
    "\n",
    "(Note: Increasing the number of hidden units and hidden layers will make model more complex. However, excessive number of hidden units and hidden layers can also hurt your model by leading it to overfit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1181a64d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfyEUpBptUKzrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116c7c5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param_hidden in param_hidden_layers:\n",
    "    regMLP = regMLPs[param_hidden]\n",
    "    plt.plot(np.sqrt(regMLP.loss_curve_), label = str(param_hidden))\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('epoch number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<learning rate: 0.005>\n",
      " Average RMSE (CV): 3.82804387751\n",
      "\n",
      "<learning rate: 0.01>\n",
      " Average RMSE (CV): 4.36444577137\n",
      "\n",
      "<learning rate: 1>\n",
      " Average RMSE (CV): 3.91685403205e+31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "param_hidden_layers = (15,)\n",
    "\n",
    "n_folds = 5\n",
    "k_folds = KFold(n_splits=n_folds)\n",
    "lrate = [0.005, 0.01, 1]\n",
    "\n",
    "for lrate_ in lrate:\n",
    "    aRMSE = 0\n",
    "    for train_idx, test_idx in k_folds.split(X_train):\n",
    "        regMLP = MLPRegressor(activation = 'tanh',\n",
    "                              solver = 'sgd',\n",
    "                              learning_rate='constant',\n",
    "                              random_state=42,\n",
    "                              batch_size=40,\n",
    "                              learning_rate_init = lrate_,\n",
    "                              hidden_layer_sizes = param_hidden)\n",
    "        stdScaler = StandardScaler()\n",
    "        xTrain = stdScaler.fit_transform(X_train.iloc[train_idx,:])\n",
    "        yTrain = y_train.iloc[train_idx]\n",
    "            \n",
    "        regMLP.fit(xTrain,yTrain)\n",
    "        aRMSE += np.sqrt(mean_squared_error(y_train.iloc[test_idx],\n",
    "                                            regMLP.predict(stdScaler.transform(X_train.iloc[test_idx,:]))))/n_folds\n",
    "    \n",
    "    print '<learning rate: {}>\\n Average RMSE (CV): {}\\n'.format(lrate_,aRMSE)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
